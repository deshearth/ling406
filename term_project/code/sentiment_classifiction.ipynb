{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk, random, json, collections, itertools\n",
    "import nltk.classify.util, nltk.metrics\n",
    "\n",
    "import operator as op\n",
    "import numpy as np\n",
    "import operator as op\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import division\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, precision, recall, f_measure\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.util import mark_negation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions  \n",
    "A class and several classmethods are defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SentiClassifier:\n",
    "    \"\"\"This is the class of sentiment classifier, \n",
    "    the corpus used is movie_reviews from nltk.corpus that containing\n",
    "    1000 negative reviews and 1000 postive reviews\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus=movie_reviews):\n",
    "        self._corpus = corpus\n",
    "        self._lemmatizer = WordNetLemmatizer()\n",
    "       \n",
    "    def preprocess(self, handle_negation=False, lemmatized=False):\n",
    "        \"\"\"\n",
    "        inputs are two options: whether handle negation and whether\n",
    "        lemmatize the words\n",
    "        return documents as dictionary, the keys are the categories\n",
    "        of the corpus, the length is equal to the number of the docs,\n",
    "        and every doc is stored as a list of ``lemmatized`` words\n",
    "        \n",
    "        \"\"\"\n",
    "        documents = {}\n",
    "        corpus = self._corpus\n",
    "        cats = corpus.categories()\n",
    "    \n",
    "        fn = (lambda x : self._lemmatizer.lemmatize(x.lower())) if lemmatized else (lambda x : x.lower())\n",
    "        \n",
    "        #documents = {cat : [map(fn, corpus.words(fileids=[_])) for _ in corpus.fileids(cat)] for cat in cats}\n",
    "        #(list comprehension is quicker than map)\n",
    "        documents = {cat : [[fn(word) for word in corpus.words(fileids=[_])] for _ in corpus.fileids(cat)] \n",
    "                     for cat in cats}\n",
    "        if handle_negation is True:\n",
    "            #documents = {cat : map(mark_negation, documents[cat]) for cat in cats}\n",
    "            documents = {cat: [mark_negation(document) for document in documents[cat]] for cat in cats}\n",
    "        return documents\n",
    "    \n",
    "    def choose_features(self, documents, (toprank, n)=(False, None), \n",
    "                        score_fn=BigramAssocMeasures.likelihood_ratio):\n",
    "        \"\"\"\n",
    "        This function provide certain number of feature based on the option.\n",
    "        `toprank` is true when informative feature is preferred\n",
    "        \n",
    "        calculate potential informative feature ahead of training.\n",
    "        scores assigned to every word is calculated by some statistical function.\n",
    "        For more details, refer to documents of `BigramAssocMeasures`\n",
    "        \"\"\"\n",
    "        cats = documents.keys()\n",
    "        #words = {cat : reduce(op.add, documents[cat]) for cat in cats}\n",
    "        # (use generator whenever possible, and don't use reduce and map if possible, two slow)\n",
    "        words = {cat : list(itertools.chain.from_iterable(documents[cat])) for cat in cats}\n",
    "        if toprank is False: return set(op.add(*words.values())[:n])\n",
    "        \n",
    "        word_freq = FreqDist(op.add(*words.itervalues()))\n",
    "        word_label_freq = ConditionalFreqDist() \n",
    "        for cat in cats:\n",
    "            word_label_freq[cat] = FreqDist(words[cat])\n",
    "            \n",
    "        \n",
    "        num_words = {cat : word_label_freq[cat].N() for cat in cats}\n",
    "        scores = {}\n",
    "        for word, freq in word_freq.iteritems():\n",
    "            pn_score = {cat : score_fn(word_label_freq[cat][word], \n",
    "                                    (freq, num_words[cat]), sum(num_words.values())) for cat in cats}\n",
    "            scores[word] = sum(pn_score.values())\n",
    "        nbestfeatures = map(op.itemgetter(0), \n",
    "                            sorted(scores.iteritems(), key=op.itemgetter(1), reverse=True))[:n]\n",
    "        return set(nbestfeatures)  \n",
    "    \n",
    "    def extract_features(self, documents, features, unif_len=False, fuse=True):\n",
    "        \"\"\"\n",
    "        `features in unigram model`\n",
    "        feature name is the word, the value is true when the word is present in that doc,\n",
    "        false otherwise. Specifically, this is the case when `unif_len` is true.\n",
    "        (`unif_len` means every document has the feature of same length.\n",
    "        By experimenting, it turns out the performance is usually worse when unif_len is true.)\n",
    "        When `unif_len` is false, it only has true value, false value is considered as\n",
    "        missing value in model training, a lot more details are explained in reports.\n",
    "        if `fuse` is true, separate different categories, store docs in dict\n",
    "        else, return a list ------------- related to evaluation methods\n",
    "        \"\"\"\n",
    "        cats = documents.keys()\n",
    "        if unif_len is True: \n",
    "            featuresets = {cat : [(dict([(word, word in set(document)) for word in features]), cat) \n",
    "                           for document in documents[cat]] for cat in cats}\n",
    "        else: \n",
    "            featuresets = {cat : [(dict([(word, True) for word in set(document) if word in features]), cat)\n",
    "                           for document in documents[cat]] for cat in cats}\n",
    "    \n",
    "        return op.add(*featuresets.values()) if fuse is True else featuresets\n",
    "    \n",
    "    def train(self, trainer, trainsets):\n",
    "        return trainer.train(trainsets)\n",
    "    \n",
    "    \n",
    "    def evaluation(self, classifier, testsets):\n",
    "        \"\"\"\n",
    "        evaluation function for fused featuresets\n",
    "        pay attention to difference from the evaluation_sepdocs\n",
    "        this function evaluates the overall documents\n",
    "        \"\"\"    \n",
    "        fvects, labels = zip(*testsets)\n",
    "        predvals = classifier.classify_many(fvects)\n",
    "        return confusion_matrix(np.array(predvals), np.array(labels))\n",
    "    \n",
    "    def cross_validation(self, trainer, featuresets, k_folds=5):\n",
    "        \"\"\"cross validation function\n",
    "        inputs: \n",
    "            `trainer` is a classification model\n",
    "            `featuresets` contains all feature set\n",
    "        output: a confusion matrix\"\"\"\n",
    "        conf_mats = np.zeros([k_folds, 2, 2])\n",
    "        test_size = len(featuresets) // k_folds\n",
    "        for i in xrange(k_folds):\n",
    "            s, e = i*test_size, (i+1)*test_size\n",
    "            testsets, trainsets = featuresets[s:e], cutlist(featuresets, s, e)\n",
    "            classifier = senti_classifier.train(NaiveBayesClassifier, trainsets)\n",
    "            conf_mats[i] = self.evaluation(classifier, testsets)\n",
    "            #print \"This is folder {}.\\n performace is {}\".format(i,metrics(conf_mats[i]))\n",
    "        return conf_mats.sum(axis=0)\n",
    "            \n",
    "    def evaluation_sepdocs(self, classifier, testsets):\n",
    "        \"\"\"\n",
    "        evaluation function for separate featuresets\n",
    "        This evaluation function evaluates the result in separate\n",
    "        categories. For example, neg and pos documents has their\n",
    "        own evaluation. \n",
    "        \"\"\"\n",
    "        reference = collections.defaultdict(set)\n",
    "        test = collections.defaultdict(set)\n",
    "        \n",
    "        for i, (featvec, trueval) in enumerate(testsets):\n",
    "            reference[trueval].add(i)\n",
    "            predval = classifier.classify(featvec)\n",
    "            test[predval].add(i)\n",
    "        \n",
    "        performance = pd.DataFrame(data=.0, index=self._corpus.categories(), \n",
    "                                   columns=['Accuracy', 'Precision', 'Recall', 'F-Measure'])\n",
    "        for idx in performance.index:\n",
    "            performance.loc[idx, 'Accuracy'] = \\\n",
    "                len(set.union(*map(set.intersection, test.values(), reference.values()))) / len(testsets)\n",
    "            performance.loc[idx, 'Precision'] = precision(reference[idx], test[idx])\n",
    "            performance.loc[idx, 'Recall'] = recall(reference[idx], test[idx])\n",
    "            performance.loc[idx, 'F-Measure'] = f_measure(reference[idx], test[idx])\n",
    "        \n",
    "        return performance\n",
    "\n",
    "######################\n",
    "# utility functions  #\n",
    "######################\n",
    "\n",
    "\n",
    "\n",
    "def metrics(conf_mat):\n",
    "    \"\"\"\n",
    "    return a tuple: (accuracy, precision, recall, f-measure)\n",
    "    accuracy (tp + tn) / (tp + fn + fp + tn)\n",
    "    precision tp / (tp + fp)\n",
    "    recall tp / (tp + fn)\n",
    "    f-measure 1.0 / (alpha / p + (1-alpha) / r), here we set alpha = 0.5\n",
    "    \"\"\"\n",
    "    accuracy = conf_mat.diagonal().sum() / conf_mat.sum()\n",
    "    details = {}\n",
    "    for i, label in enumerate(['pos','neg']):\n",
    "        p = conf_mat[i,i] / conf_mat[:,i].sum()\n",
    "        r = conf_mat[i,i] / conf_mat[i,:].sum()\n",
    "        f_measure = 2 * (r*p) / (r+p) \n",
    "        details[label] = np.array((p, r, f_measure))\n",
    "       \n",
    "    details['avg'] = (details['pos'] + details['neg']) / 2\n",
    "    return accuracy, details\n",
    "    \n",
    "def confusion_matrix(predvals, labels):\n",
    "    \"\"\"\n",
    "                predicted values\n",
    "             ------------------\n",
    "            |     pos  |   neg |\n",
    "      true  |----------------- |\n",
    "    values  |pos| tp   |  fn   |\n",
    "            |------------------|\n",
    "            |neg| fp   |  tn   |\n",
    "            --------------------\n",
    "    \"\"\"\n",
    "    conf_mat = np.zeros((2,2))\n",
    "    # true positive\n",
    "    conf_mat[0,0] = (np.logical_and(labels==u'pos', predvals==u'pos')).sum()\n",
    "    # false negative\n",
    "    conf_mat[0,1] = (np.logical_and(labels==u'pos', predvals==u'neg')).sum()\n",
    "    # false positive\n",
    "    conf_mat[1,0] = (np.logical_and(labels==u'neg', predvals==u'pos')).sum()\n",
    "    # true negative\n",
    "    conf_mat[1,1] = (np.logical_and(labels==u'neg', predvals==u'neg')).sum()\n",
    "    return conf_mat\n",
    "\n",
    "def cutlist(seq, s, e):\n",
    "    return seq[:s] + seq[e:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Script \n",
    "(explore the promising feature sets by changing parameters)  \n",
    "parameters you may want to change:  \n",
    "  \n",
    "*handle_negation*: True if negation handling is desired  \n",
    "*lemmatized*: True if lemmatization is desired  \n",
    "*score_fn*: three functions discussed in report likelihood_ratio, chi_sq, phi_sq,   \n",
    "*toprank, n*: toprank is True if more informative features are desired, n is the feature words to use,   \n",
    "*uni_len*: True if uniform feature is desired  \n",
    "*cutoff*: the value is equal to #(training instances) / #(all instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "senti_classifier = SentiClassifier()\n",
    "documents = senti_classifier.preprocess(handle_negation=True,lemmatized=False)\n",
    "toprank, n = (True, 5000)\n",
    "nbestfeatures = senti_classifier.choose_features(documents, (toprank, n), \\\n",
    "                                                       score_fn=BigramAssocMeasures.likelihood_ratio)\n",
    "featuresets = senti_classifier.extract_features(documents, nbestfeatures, unif_len=False)\n",
    "random.seed(78)\n",
    "random.shuffle(featuresets)\n",
    "cutoff = len(featuresets) // 5\n",
    "trainsets, testsets = featuresets[cutoff:], featuresets[:cutoff]\n",
    "trainer = NaiveBayesClassifier\n",
    "classifier = senti_classifier.train(trainer, trainsets)\n",
    "\n",
    "print metrics(senti_classifier.evaluation(classifier, testsets))\n",
    "print metrics(senti_classifier.cross_validation(classifier, featuresets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment (on Influence of length of Feature sets)  \n",
    "You may want to change *ss*, *end*, *ss*  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senti_classifier = SentiClassifier()\n",
    "documents = senti_classifier.preprocess(handle_negation=True,lemmatized=False)\n",
    "results = []\n",
    "\n",
    "s = 500\n",
    "end = 20001\n",
    "ss = 500\n",
    "\n",
    "for i in xrange(s, end, ss):\n",
    "    \n",
    "    nbestfeatures = senti_classifier.choose_features(documents, (True, i), \\\n",
    "                                                           score_fn=BigramAssocMeasures.likelihood_ratio)\n",
    "    featuresets = senti_classifier.extract_features(documents, nbestfeatures, unif_len=False)\n",
    "    random.seed(78)\n",
    "    random.shuffle(featuresets)\n",
    "    cutoff = len(featuresets) // 5\n",
    "    trainsets, testsets = featuresets[cutoff:], featuresets[:cutoff]\n",
    "    trainer = NaiveBayesClassifier\n",
    "    classifier = senti_classifier.train(trainer, trainsets)\n",
    "\n",
    "    #print metrics(senti_classifier.evaluation(classifier, testsets))\n",
    "    result = metrics(senti_classifier.cross_validation(classifier, featuresets))\n",
    "    results.append(result)\n",
    "    print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the results from previous cell. So make sure you run the previous cell before running this\n",
    "accuracies, details = zip(*results)\n",
    "\n",
    "avgs = np.array([detail['avg'] for detail in details])\n",
    "negs = np.array([detail['neg'] for detail in details])\n",
    "poss = np.array([detail['pos'] for detail in details])\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "number = np.arange(500,20001,500)\n",
    "f_avg = plt.figure()\n",
    "plt.plot(number, avgs[:,0],'r-x', label='precision')\n",
    "plt.plot(number, avgs[:,1], 'c-^',label='recall')\n",
    "plt.plot(number, avgs[:,2], 'k-.',label='f-measure')\n",
    "plt.axis([0,20000,0.8,0.97])\n",
    "plt.title('Averaged Performance versus Number of Features')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Performance')\n",
    "plt.savefig('avg.png',format='png', dpi=1000)\n",
    "\n",
    "f_neg = plt.figure()\n",
    "plt.plot(number, negs[:,0],'r-x', label='precision')\n",
    "plt.plot(number, negs[:,1], 'c-^',label='recall')\n",
    "plt.plot(number, negs[:,2], 'k-.',label='f-measure')\n",
    "plt.axis([0,20000,0.74,1])\n",
    "plt.title('Performance on Negative Class versus Number of Features')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Performance')\n",
    "plt.savefig('neg.png',format='png', dpi=1000)\n",
    "\n",
    "f_neg = plt.figure()\n",
    "plt.plot(number, poss[:,0],'r-x', label='precision')\n",
    "plt.plot(number, poss[:,1], 'c-^',label='recall')\n",
    "plt.plot(number, poss[:,2], 'k-.',label='f-measure')\n",
    "plt.axis([0,20000,0.74,1])\n",
    "plt.title('Performance on Positive Class versus Number of Features')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Performance')\n",
    "plt.savefig('pos.png',format='png', dpi=1000)\n",
    "\n",
    "f_neg = plt.figure()\n",
    "plt.plot(number, accuracies,'g-.')\n",
    "\n",
    "plt.axis([0,20000,0.8,0.97])\n",
    "plt.title('Overall Accuracy versus Number of Features')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Performance')\n",
    "plt.savefig('accuracy.png',format='png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
